---
title: "Day 4 Lecture"
author: "Cecilia Y. Sui"
date: "01/13/2022"
output: pdf_document
---

```{r setup, include=FALSE}
library(knitr)
knitr::opts_chunk$set(echo = TRUE)
hook_output <- knit_hooks$get("output")
knit_hooks$set(output = function(x, options) {
  lines <- options$output.lines
  if (is.null(lines)) {
    return(hook_output(x, options)) # pass to default hook
  }
  x <- unlist(strsplit(x, "\n"))
  more <- "..."
  if (length(lines) == 1) { # first n lines
    if (length(x) > lines) {
      # truncate the output, but add ....
      x <- c(head(x, lines), more)
    }
  } else {
    x <- c(more, x[lines], more)
  }
  # paste these lines together
  x <- paste(c(x, ""), collapse = "\n")
  hook_output(x, options)
})
```

# Day 4 Outline:
1. Types of Statistical Data (numerical, categorical, ordinal)
2. Probability distributions:  dnorm / pnorm / qnorm / rnorm
3. Descriptive Statistics

# 1. Types of Statistical Data in R
"The most common variables used in data analysis can be classified as one of
three types of variables: numerical, categorical/nominal and ordinal data.

Understanding the differences in these types of variables is critical, since the
variable type will determine which statistical analysis will be valid for that
data." In addition, the way we summarize data with statistics and plots will be
determined by the variable type.

## 1.1 Numerical Data (Continuous and Discrete)

"Numerical data are usually from interval or ratio variables that are measured or
counted values, such as age, height, weight, number of votes. The interval
between numbers is known to be equal.

Numerical data can be either discrete or continuous. Counted data are usually
discrete, while measured data are usually continuous."

## 1.2 Categorical Data (Unordered Categorical, Nominal)

Categorical or nominal variables are data whose levels are labels or
descriptions, and which cannot be ordered. "Examples of categorical variables are
"sex, school, and yes/no" questions." 

They are also called "“nominal categorical” or “qualitative” variables, and the levels of a variable are sometimes called
“classes” or “groups”."

"The levels of categorical variables cannot be ordered. For the variable sex, it
makes no sense to try to put the levels “female”, “male”, and “other” in any
numerical order. If levels are numbered for convenience, the numbers are
arbitrary, and the variable can’t be treated as a numeric variable. Moreover,
taking the average of such numbers is often not meaningful."

## 1.3 Ordinal Data (Ordered Categorical)

"Ordinal variables can be ordered, or ranked in logical order, but the interval
between levels of the variables are not necessarily known. Subjective
measurements are often ordinal variables. One example would be having people
rank four items by preference in order from one to four." A different example
would be having people assess several items based on a Likert ranking scale: "On
a scale of one to five, do you agree or disagree with this statement?" A third
example is "level of education" for adults, considering for example "less than
high school", "high school", "associate's degree," etc.

Critically, in each case we can "order the responses, but we cannot know if the
interval between the levels is equal. For example, the distance between your
favorite salad dressing and your second favorite salad dressing may be small,
where there may be a large gap between your second and third choices."

We can logically assign numbers to levels of an ordinal variable, and can treat
them in order, "but should NOT treat them as numeric: "strongly agree" and
"neutral" may not average out to an "agree.""

For the purpose of the bootcamp,we will consider such Likert item data to be
ordinal data under most circumstances. Ordinal data is sometimes called “ordered
categorical”.

## 1.4 Types of Statistical Data in R

Unfortunately, R does not use the terms numerical, categorical, and ordinal for
types of variables.

"Numerical variables can be coded as variables with numeric or integer classes."
The default class to store numbers in R is numeric. An L used with values to
tell R to store the data as an integer class.
```{r}
Count <- c(1, 2, 3, 4, 5)
Count
class(Count)
typeof(Count)
Count.int <- c(1L, 2L, 3L, 4L, 5L)
class(Count.int)
typeof(Count.int)
```

R has two names for its floating-point vector: "double" and "numeric". 

"double" is the name of the type, and "numeric" is the name of the class. 

In R, categorical or nominal variables can be coded as variables with factor or
character classes. The **factor()** command is used to create and modify factors
in R.

For example: 
```{r}
Colors <- c("Red", "Green", "Blue")
class(Colors)

Colors.f <- factor(c("Red", "Green", "Blue"))
typeof(Colors.f) # assign integer to the unique value 
class(Colors.f)
```

Factors are stored as integers in R, and have labels associated with these
unique integers. While factors look and often behave like character vectors,
they are actually integers under the hood, and you need to be careful when
treating them like strings.

Once created, factors can only contain a **pre-defined set values**, which are
the **levels**. By default, R always sorts levels in **alphabetical** order. For
example:
```{r}
sex <- factor(c("male", "female", "female", "male"))
levels(sex)
```

R will assign 1 to the level "female" and 2 to the level "male" (because f comes
before m alphabetically, even though the first element in this vector is
"male"). You can check this by using the function **levels()**, and check the
number of levels using **nlevels()**:

```{r}
levels(sex)
nlevels(sex)
```

Sometimes, the order of the factors does not matter, other times you might want
to specify the order because it is meaningful (e.g., “low”, “medium”, “high”) or
it is required by particular type of analysis. Additionally, specifying the
order of the levels allows us to compare levels:

```{r}
TFR.f <- factor(c("low", "high", "medium", "high", "low", "medium", "high"))
# typeof(TFR.f)
# class(TFR.f)
TFR.f
levels(TFR.f)

TFR.f1 <- factor(TFR.f, levels = c("low", "medium", "high"))
levels(TFR.f1)
```

However, once you have ordered your levels, it would be great if you can find
the lowest or highest level.
```{r eval = F}
min(TFR.f1) # does not work
```
You can address that error by adding another argument in your **factor()**
function:
```{r}
TFR.f1 <- factor(TFR.f, levels = c("low", "medium", "high"), ordered = TRUE)
min(TFR.f1)
max(TFR.f1)
```

In R’s memory, these factors are represented by numbers (1, 2, 3). They are
better than using simple integer labels because factors are self describing:
"low", "medium", and "high" is more descriptive than 1, 2, 3. Which is low? You
wouldn’t be able to tell with just integer data. Factors have this information
built in. It is particularly helpful when there are many levels.

We can code ordinal data as either numeric or factor variables, depending on how
we will be summarizing, plotting, and analyzing it.

When we load a dataset into R using the read.csv function, R does not always
store them with the desired classes. For example, in our school location
dataset, R stores the column "STATE" as characters. It would make more sense to
treat them as factors. We can convert them to factors like this:

```{r tidy = TRUE}
school_loc <- read.csv("school_loc.csv")
summary(school_loc)
str(school_loc)
# From the output, we can see that R stores the states as characters.
school_loc$STATE <- factor(school_loc$STATE)
# Let's check again. You can see that STATE is now stored as factors in R.
str(school_loc)
# View(school_loc)
levels(school_loc$STATE)
# school_loc$STATE
summary(school_loc)
```

Notice the **summary()** function handles factors differently to numbers (and
strings), the occurrence counts for each value is often more useful information.

The function **table()** is also useful for viewing the observations.
```{r}
table(school_loc$STATE)
```

## \textcolor{red}{In-class exercises 4.1:}

1. Load the worldTFR dataset.
2. Get the structure of the dataframe.
3. Get the summary statistics of the dataframe. 
4. What data types are each variable stored as? 
5. Convert variables to factor as appropriate. 

# 2. Probability distributions in R

Every distribution has four associated functions whose prefix indicates the type
of function and the suffix indicates the distribution. In our examples, we will
focus on the normal (Gaussian) distribution. We will not cover the mathematical
concepts in depth for normal distribution. You will learn more about it in both
QPM I and QPM II.

The four normal distribution functions are: 

* dnorm: density function of the normal distribution
* pnorm: cumulative density function of the normal distribution
* qnorm: quantile function of the normal distribution
* rnorm: random sampling from the normal distribution

## 2.1 dnorm 

"x is the value(s) for which you want to find the probability density."
"output of dnorm(x) is the value of the probability density function (PDF) of the normal distribution at the points specified by x." (likelihood)
"dnorm(x) tells you the "height" of the normal distribution curve at that point."

```{r}
?dnorm
dnorm(0:10, mean = 0, sd = 1, log = FALSE)
```


Using the density, it is possible to determine the probabilities of events. 

For example, you may wonder: "What is the likelihood that a person has an IQ of
exactly 140?." In this case, you would need to retrieve the density of the IQ
distribution at value 140. The IQ distribution can be modeled with a mean of 100
and a standard deviation of 15. The corresponding density is:

```{r}
sample.range <- 50:150
iq.mean <- 100
iq.sd <- 15

# This returns the density, NOT the values for IQ
iq.dist <- dnorm(sample.range, mean = iq.mean, sd = iq.sd)
iq.dist
iq.df <- data.frame("IQ" = sample.range, "Density" = iq.dist)
View(iq.df)
plot(iq.df)

# plot(iq.df$IQ, iq.df$Density , type = "l", main = "Density of Normal Distribution", xlab = "IQ", ylab = "Density")

```
From these data, we can now answer the initial question as well as additional
questions. Let's write a function that returns a percentage value when given a
density.
```{r}
# print percentage value when given density
pp <- function(x) {
  print(paste0(round(x * 100, 3), "%")) # concatenate
}

# ?paste0

# likelihood of IQ == 140?
pp(iq.df$Density[iq.df$IQ == 140])

# likelihood of IQ >= 140?
pp(sum(iq.df$Density[iq.df$IQ >= 140]))

# likelihood of 50 < IQ <= 90?
pp(sum(iq.df$Density[iq.df$IQ <= 90]))

# What does this code do?
pp(sum(iq.df$Density[iq.df$IQ <= 100]) + sum(iq.df$Density[iq.df$IQ > 100]))
```
"riemann sum"

## 2.2 pnorm 
"The cumulative density (CDF) function is a monotonically increasing function as
it integrates over densities." To get an intuition of the CDF, let’s create a
plot for the IQ data:
```{r}
cdf <- pnorm(sample.range, iq.mean, iq.sd)
cdf
?pnorm
iq.df <- cbind(iq.df, "CDF_LowerTail" = cdf)
# Can you explain what this code above is doing?
View(iq.df)

# visual aid of pdf
plot(50:150, dnorm(50:150, 100, 15), type = 'l')
abline(v = 80, col = 'red')

# visual aid of cdf
library(ggplot2)
ggplot(iq.df, aes(x = IQ, y = CDF_LowerTail)) +
  geom_point()
```

As we can see, the depicted CDF shows the probability of having an IQ less or
equal to a given value. This is because pnorm computes the lower tail by
default, i.e. $P[X<=x]$, as we see in the help pages. Using this knowledge, we
can obtain answers to some of our previous questions in a slightly different
manner.
```{r}
# likelihood of IQ <= 140?
# notice here we using == signs
pp(iq.df$CDF_LowerTail[iq.df$IQ == 140])

# likelihood of IQ <= 90?
pp(iq.df$CDF_LowerTail[iq.df$IQ ==  90])

plot(50:150, dnorm(50:150, 100, 15), type = 'l')
abline(v = 90, col = 'red')

# set lower.tail to FALSE to obtain P[X >= x]
# upper.tail from the sample range
cdf <- pnorm(sample.range, iq.mean, iq.sd, lower.tail = FALSE)
iq.df <- cbind(iq.df, "CDF_UpperTail" = cdf)
View(iq.df)
# Probability for IQ >= 140? same value as before using dnorm!
pp(iq.df$CDF_UpperTail[iq.df$IQ == 140])
pp(iq.df$CDF_LowerTail[iq.df$IQ == 140])

iq.df$CDF_LowerTail[iq.df$IQ == 140] + iq.df$CDF_UpperTail[iq.df$IQ == 140] 

plot(50:150, dnorm(50:150, 100, 15), type = 'l')
points(140, dnorm(140, 100, 15), col = 'red')

```

Note that the results from pnorm are the same as those obtained from manually
summing up the probabilities obtained via dnorm. Moreover, by setting
**lower.tail = FALSE**, dnorm can be used to directly compute **p-values**,
which measure how the likelihood of an observation that is at least as extreme
as the obtained one.

"It is important to remember that **pnorm** does NOT provide the PDF but the
**CDF**."

## 2.3 qnorm 
The quantile function is simply the inverse of the cumulative density function
(iCDF). Thus, the quantile function maps from probabilities to values.

Let’s take a look at the quantile function for $P[X<=x]$:

```{r}
# input to qnorm is a vector of probabilities
prob.range <- seq(0, 1, 0.001)
prob.range
# ?seq

# inverse cdf
icdf.df <- data.frame("Probability" = prob.range, "IQ" = qnorm(prob.range, iq.mean, iq.sd))
View(icdf.df)
ggplot(icdf.df, aes(x = Probability, y = IQ)) +
  geom_point()
```

As illustrated with the plot, given a specific probablity, you can match it back
to IQ values.

```{r}
# Using the quantile function, we can answer quantile-related questions:
# what is the 25th IQ percentile? (Q1)
print(icdf.df$IQ[icdf.df$Probability == 0.25])

# What is the 50th IQ percentile? (median or Q2)
# It's also the mean in normal distributions.
print(icdf.df$IQ[icdf.df$Probability == 0.5])

# what is the 75th IQ percentile? (Q3)
print(icdf.df$IQ[icdf.df$Probability == 0.75])

# note: this is the same results as from the quantile function
quantile(icdf.df$IQ)

# what is the 90th IQ percentile?
print(icdf.df$IQ[icdf.df$Probability == 0.9])
```

## 2.4 rnorm 

When you want to draw random samples from the normal distribution, you can use
**rnorm.**

For example, we could use **rnorm** to simulate random samples from the IQ
distribution.

```{r}
# fix random seed for reproducibility
set.seed(1234)

?rnorm
obs <- rnorm(n = 100, iq.mean, iq.sd)
mean(obs)
obs

## More Advanced Content:
## Don't worry if the code below does not totally make sense so far.
# law of large numbers: mean will approach expected value for large N
n.samples <- c(100, 1000, 10000)

my.df <- do.call(rbind, lapply(
  n.samples,
  function(x) {
    data.frame(
      "SampleSize" = x,
      "IQ" = rnorm(x, iq.mean, iq.sd)
    )
  }
))

# do.call is just like python * (the function(x) returns list and do.call removes the list since rbind only need the df as an element not dfs in the list)

# show one facet per random sample of a given size
ggplot() +
  geom_histogram(data = my.df, aes(x = IQ)) +
  facet_wrap(. ~ SampleSize, scales = "free_y")

# note: we can also implement our own sampler using the densities
my.sample <- sample(iq.df$IQ, 100, prob = iq.df$Density, replace = TRUE)
my.sample.df <- data.frame("IQ" = my.sample)
ggplot(my.sample.df, aes(x = IQ)) +
  geom_histogram()
```

Note that we called **set.seed()** in order to ensure that the random number
generator always generates the same sequence of numbers for reproducibility.

Of the four functions dealing with distributions, **dnorm** is the most
important one. This is because the values from pnorm, qnorm, and rnorm are based
on dnorm. Still, pnorm, qnorm, and rnorm are very useful convenience functions
when dealing with the normal distribution. If you would like to learn about the
corresponding functions for the other distributions, you can simply call
**?distribtuion** to obtain more information.

## \textcolor{red}{In-class exercises 4.2:}

Please explain what the following code does:

1.
```{r}
pnorm(0)
?pnorm
```

2. 
```{r}
pnorm(1.96, lower.tail = TRUE)
pnorm(1.96, lower.tail = FALSE)
```

3.
```{r}
dnorm(0:10, mean = 0, sd = 1, log = FALSE)
```

4.
```{r}
qnorm(0.025)
```

5. 
```{r}
qnorm(0.975)
```

6. 
```{r}
qnorm(.975, 20, 1.65)
```

7. 
```{r}
mean(rnorm(1:1000, 0.05, 0.5))
```

# 3. Descriptive Statistics

## 3.1 Name Commands 
* names() – It works on matrix or data frame objects.
```{r}
names(school_loc)
```

* rownames() – It works on matrix or data frame objects and is used to give
  names to rows.
```{r output.lines = 10}
rownames(school_loc)
```

* colnames() – It works on matrix or data frame objects and is used to give
  names to columns.
```{r}
colnames(school_loc)
```

* dimnames() – Gets row and column names for matrix or data frame objects, that
  is, it is used to see dimensions of the data frame.
```{r output.lines=5}
dimnames(school_loc)[2]
dim(school_loc)
```

```{r output.lines=10}
dimnames(school_loc)[2]
```

## 3.2 Summarizing Commands 

* max(x, na.rm = FALSE) – It shows the maximum value. By default, NA values are
  not removed. NA is considered the largest unless na.rm=true is used.
* min(x, na.rm = FALSE) – Shows minimum value in a vector. If there are na
  values, NA is returned unless na.rm=true is used.
* length(x) – Gives length of the vector and includes na values.
  Na.rm=instruction does not work with this command.
* sum(x, na.rm = FALSE) – Shows the sum of the vector elements.
* mean(x, na.rm = FALSE) – We obtain an arithmetic mean with this.
* median( x, na.rm = FALSE) – Shows the median value of the vector.
* sd(x, na.rm = FALSE) – Shows the standard deviation.
* var(x, na.rm = FALSE) – Shows the variance.

* mad(x, na.rm = FALSE) – Shows the median absolute deviation.
This first finds the median of the data set, then computes the absolute differences between each data point and the median, and finally takes the median of these absolute differences.
* log(dataset) – Shows log value for each element.
* summary(dataset) – We have seen how it shows a summary of dataset like maximum
  value, minimum value, mean, etc.
* quantile() – Shows the quantiles by default—the 0%, 25%, 50%, 75%, and 100%
  quantiles. You can select other quantiles also.
```{r}
x <- sample(1:100000, 2000)
quantile(x, probs = seq(0, 1, 0.1), na.rm = FALSE, names = TRUE)
?quantile
```

We will do more exercises where you would be asked to find these summarizing
statistics given a dataframe in the afternoon session.

## 3.3 Accessing Elements from Summary Statistics

You might want to access elements from the summary statistics sometimes. We will
use the built-in dataset longley again to show some examples.
```{r}
df <- longley
View(longley)
summary(df)
s <- summary(df)
s
# You can simply use indices to access each element like this:
s[7]
typeof(s)
```

When we go into linear regression in QPM I, accessing elements of list-like
objects would become more useful. For example, we can fit a linear model using
the lm() function where the independent variable is population and the dependent
variable is the number of unemployed.
```{r}
lm <- lm(Unemployed ~ Population, data = df) # regressing Unemp on Pop
lm
```

Let's check the structure of the object: 
```{r output.lines=10}
str(lm)
```

You can use the summary() function again on lm. 
```{r output.lines=10}
summary(lm)
```

If you want to access elements from the summary statistics, you can use dollar
signs and brackets, just like we did on Day 2.  
```{r}
lm$coefficients
lm$residuals
lm$coefficients[1]
lm$coefficients[[1]]
```

## \textcolor{red}{In-class exercises 4.3:}

We will continue using the worldTFR dataset.

1. Load the dataset and omit all NA's in the dataframe. 
2. Run a linear regression of ChildBearing on TFR.
3. Get the coefficients. 
4. What are the column names and row names? 
5. What is the max and min of Years? 
6. What is the average Life Expectancy at Birth (LifeExpB)?
7. What is the median Life Expectancy at Birth (LifeExpB)?
8. What is the 25% and 75% quantile of Life Expectancy at Birth (LifeExpB)?

```{r eval = FALSE}
# 1.
worldTFR <- read.csv("worldTFR.csv")
TFR <- na.omit(worldTFR)

# 2.
ols <- lm(TFR ~ ChildBearing, data = TFR)

# 3.
ols$coefficients

# 4.
colnames(TFR)
rownames(TFR)

# 5.
max(TFR$Year)
min(TFR$Year)

# 6.
mean(TFR$LifeExpB)

# 7.
median(TFR$LifeExpB)

# 8.
quantile(TFR$LifeExpB)
```
