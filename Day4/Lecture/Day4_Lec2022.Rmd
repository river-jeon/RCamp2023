---
title: "Day 4 Lecture"
author: "Cecilia Y. Sui"
date: "01/13/2022"
output: pdf_document
---

```{r setup, include=FALSE}
library(knitr)
knitr::opts_chunk$set(echo = TRUE)
hook_output <- knit_hooks$get("output")
knit_hooks$set(output = function(x, options) {
  lines <- options$output.lines
  if (is.null(lines)) {
    return(hook_output(x, options))  # pass to default hook
  }
  x <- unlist(strsplit(x, "\n"))
  more <- "..."
  if (length(lines)==1) {        # first n lines
    if (length(x) > lines) {
      # truncate the output, but add ....
      x <- c(head(x, lines), more)
    }
  } else {
    x <- c(more, x[lines], more)
  }
  # paste these lines together
  x <- paste(c(x, ""), collapse = "\n")
  hook_output(x, options)
})
```


# Day 4 Outline:
1. Types of Statistical Data (numerical, categorical, ordinal)
2. Probability distributions: rnorm / dnorm / pnorm / qnorm
3. Descriptive Statistics

# 1. Types of Statistical Data in R
The most common variables used in data analysis can be classified as one of three types of variables: numerical, categorical/nominal and ordinal data. 

Understanding the differences in these types of variables is critical, since the variable type will determine which statistical analysis will be valid for that data.  In addition, the way we summarize data with statistics and plots will be determined by the variable type.

## 1.1 Numerical Data 

Numerical data are usually from interval or ratio variables that are measured or counted values, such as age, height, weight, number of votes. The interval between numbers is known to be equal. 

Numerical data can be either discrete or continuous. Counted data are usually discrete, while measured data are usually continuous. 

## 1.2 Categorical Data 

Categorical or nominal variables are data whose levels are labels or descriptions, and which cannot be ordered. Examples of categorical variables are sex, school, and yes/no questions.They are also called “nominal categorical” or “qualitative” variables, and the levels of a variable are sometimes called “classes” or “groups”.

The levels of categorical variables cannot be ordered. For the variable sex, it makes no sense to try to put the levels “female”, “male”, and “other” in any numerical order. If levels are numbered for convenience, the numbers are arbitrary, and the variable can’t be treated as a numeric variable. Moreover, taking the average of such numbers is often not meaningful. 

## 1.3 Ordinal Data 

Ordinal variables can be ordered, or ranked in logical order, but the interval between levels of the variables are not necessarily known. Subjective measurements are often ordinal variables. One example would be having people rank four items by preference in order from one to four. A different example would be having people assess several items based on a Likert ranking scale: "On a scale of one to five, do you agree or disagree with this statement?" A third example is level of education for adults, considering for example "less than high school", "high school", "associate's degree," etc. 

Critically, in each case we can order the responses, but we cannot know if the interval between the levels is equal. For example, the distance between your favorite salad dressing and your second favorite salad dressing may be small, where there may be a large gap between your second and third choices.

We can logically assign numbers to levels of an ordinal variable, and can treat them in order, but should NOT treat them as numeric: "strongly agree" and "neutral" may not average out to an "agree."

For the purpose of the bootcamp,we will consider such Likert item data to be ordinal data under most circumstances. Ordinal data is sometimes called “ordered categorical”.


## 1.4 Types of Statistical Data in R

Unfortunately, R does not use the terms numerical, categorical, and ordinal for types of variables.

Numerical variables can be coded as variables with numeric or integer classes. The default class to store numbers in R is numeric. An L used with values to tell R to store the data as an integer class.
```{r}
Count <- c(1, 2, 3, 4, 5)
class(Count)
Count.int <- c(1L, 2L, 3L, 4L, 5L)
class(Count.int)
```

R has two names for its floating-point vector: "double" and "numeric". 

"double" is the name of the type, and "numeric" is the name of the class. 


In R, categorical or nominal variables can be coded as variables with factor or character classes. The **factor()** command is used to create and modify factors in R.

For example: 
```{r}
Colors <- c("Red", "Green", "Blue")
class(Colors)
Colors.f <- factor(c("Red", "Green", "Blue"))
class(Colors.f)
```


Factors are stored as integers in R, and have labels associated with these unique integers. While factors look and often behave like character vectors, they are actually integers under the hood, and you need to be careful when treating them like strings. 

Once created, factors can only contain a **pre-defined set values**, which are the **levels**. By default, R always sorts levels in **alphabetical** order. For example: 
```{r}
sex <- factor(c("male", "female", "female", "male"))
levels(sex)
```

R will assign 1 to the level "female" and 2 to the level "male" (because f comes before m alphabetically, even though the first element in this vector is "male"). You can check this by using the function **levels()**, and check the number of levels using **nlevels()**:

```{r}
levels(sex)
nlevels(sex)
```

Sometimes, the order of the factors does not matter, other times you might want to specify the order because it is meaningful (e.g., “low”, “medium”, “high”) or it is required by particular type of analysis. Additionally, specifying the order of the levels allows us to compare levels:

```{r}
TFR.f <- factor(c("low", "high", "medium", "high", "low", "medium", "high"))
levels(TFR.f)

TFR.f1 <- factor(TFR.f, levels=c("low", "medium", "high"))
levels(TFR.f1)
```

However, once you have ordered your levels, it would be great if you can find the lowest or highest level. 
```{r eval = F}
min(TFR.f1) # does not work
```
You can address that error by adding another argument in your **factor()** function: 
```{r}
TFR.f1 <- factor(TFR.f, levels=c("low", "medium", "high"), ordered = TRUE)
min(TFR.f1)
max(TFR.f1)
```

In R’s memory, these factors are represented by numbers (1, 2, 3). They are better than using simple integer labels because factors are self describing: "low", "medium", and "high"" is more descriptive than 1, 2, 3. Which is low? You wouldn’t be able to tell with just integer data. Factors have this information built in. It is particularly helpful when there are many levels. 

We can code ordinal data as either numeric or factor variables, depending on how we will be summarizing, plotting, and analyzing it.





When we load a dataset into R using the read.csv function, R does not always store them with the desired classes. For example, in our school location dataset, R stores the column "STATE" as characters. It would make more sense to treat them as factors. We can convert them to factors like this: 

```{r tidy = TRUE}
school_loc <- read.csv("school_loc.csv")
# summary(school_loc)
str(school_loc)
# From the output, we can see that R stores the states as characters. 
school_loc$STATE = factor(school_loc$STATE)
# Let's check again. You can see that STATE is now stored as factors in R. 
str(school_loc)

summary(school_loc)
```

Notice the **summary()** function handles factors differently to numbers (and strings), the occurrence counts for each value is often more useful information.

The function **table()** is also useful for viewing the observations. 
```{r}
table(school_loc$STATE)
```



## \textcolor{red}{In-class exercises 4.1:}

1. Load the worldTFR dataset.
2. Get the structure of the dataframe.
3. Get the summary statistics of the dataframe. 
4. What data types are each variable stored as? 
5. Convert variables to factor as appropriate. 


# 2. Probability distributions in R

Every distribution has four associated functions whose prefix indicates the type of function and the suffix indicates the distribution. In our examples, we will focus on the normal (Gaussian) distribution. We will not cover the mathematical concepts in depth for normal distribution. You will learn more about it in both QPM I and QPM II. 

The four normal distribution functions are: 

* dnorm: density function of the normal distribution
* pnorm: cumulative density function of the normal distribution
* qnorm: quantile function of the normal distribution
* rnorm: random sampling from the normal distribution

## 2.1 dnorm 
```{r}
# ?dnorm
dnorm(0:10, mean = 0, sd = 1, log = FALSE)
```

Using the density, it is possible to determine the probabilities of events. 

For example, you may wonder: What is the likelihood that a person has an IQ of exactly 140?. In this case, you would need to retrieve the density of the IQ distribution at value 140. The IQ distribution can be modeled with a mean of 100 and a standard deviation of 15. The corresponding density is:

```{r}
sample.range <- 50:150
iq.mean <- 100
iq.sd <- 15

# This returns the density, NOT the values for IQ
iq.dist <- dnorm(sample.range, mean = iq.mean, sd = iq.sd) 
iq.df <- data.frame("IQ" = sample.range, "Density" = iq.dist)
plot(iq.df)
```
From these data, we can now answer the initial question as well as additional questions. Let's write a function that returns a percentage value when given a density.
```{r}
pp <- function(x) {
    print(paste0(round(x * 100, 3), "%"))
}

# ?paste0

# likelihood of IQ == 140?
pp(iq.df$Density[iq.df$IQ == 140])

# likelihood of IQ >= 140?
pp(sum(iq.df$Density[iq.df$IQ >= 140]))

# likelihood of 50 < IQ <= 90?
pp(sum(iq.df$Density[iq.df$IQ <= 90]))

# What does this code do?
pp(sum(iq.df$Density[iq.df$IQ <= 100]) + sum(iq.df$Density[iq.df$IQ > 100]))
```



## 2.2 pnorm 
The cumulative density (CDF) function is a monotonically increasing function as it integrates over densities.
To get an intuition of the CDF, let’s create a plot for the IQ data:
```{r}
cdf <- pnorm(sample.range, iq.mean, iq.sd)
iq.df <- cbind(iq.df, "CDF_LowerTail" = cdf) 
# Can you explain what this code above is doing?

library(ggplot2)
ggplot(iq.df, aes(x = IQ, y = CDF_LowerTail)) + geom_point()
```



As we can see, the depicted CDF shows the probability of having an IQ less or equal to a given value. This is because pnorm computes the lower tail by default, i.e. $P[X<=x]$, as we see in the help pages. Using this knowledge, we can obtain answers to some of our previous questions in a slightly different manner. 
```{r}
# likelihood of IQ <= 140?
# notice here we using == signs
pp(iq.df$CDF_LowerTail[iq.df$IQ == 140]) 

# likelihood of 50 < IQ <= 90?
pp(iq.df$CDF_LowerTail[iq.df$IQ == 90])

# set lower.tail to FALSE to obtain P[X >= x]
cdf <- pnorm(sample.range, iq.mean, iq.sd, lower.tail = FALSE)
iq.df <- cbind(iq.df, "CDF_UpperTail" = cdf)
# Probability for IQ >= 140? same value as before using dnorm!
pp(iq.df$CDF_UpperTail[iq.df$IQ == 140])
```

Note that the results from pnorm are the same as those obtained from manually summing up the probabilities obtained via dnorm. Moreover, by setting **lower.tail = FALSE**, dnorm can be used to directly compute **p-values**, which measure how the likelihood of an observation that is at least as extreme as the obtained one. 

It is important to remember that **pnorm** does NOT provide the PDF but the **CDF**.


## 2.3 qnorm 
The quantile function is simply the inverse of the cumulative density function (iCDF). Thus, the quantile function maps from probabilities to values. 

Let’s take a look at the quantile function for $P[X<=x]$:

```{r}
# input to qnorm is a vector of probabilities
prob.range <- seq(0, 1, 0.001) 
# ?seq

icdf.df <- data.frame("Probability" = prob.range, "IQ" = qnorm(prob.range, iq.mean, iq.sd))
# View(icdf.df)
ggplot(icdf.df, aes(x = Probability, y = IQ)) + geom_point()
```

As illustrated with the plot, given a specific probablity, you can match it back to IQ values.

```{r}
# Using the quantile function, we can answer quantile-related questions:
# what is the 25th IQ percentile? (Q1)
print(icdf.df$IQ[icdf.df$Probability == 0.25])

# What is the 50th IQ percentile? (median or Q2) 
# It's also the mean in normal distributions. 
print(icdf.df$IQ[icdf.df$Probability == 0.5])

# what is the 75th IQ percentile? (Q3) 
print(icdf.df$IQ[icdf.df$Probability == 0.75])

# note: this is the same results as from the quantile function
quantile(icdf.df$IQ)

# what is the 90th IQ percentile?
print(icdf.df$IQ[icdf.df$Probability == 0.9])
```





## 2.4 rnorm 

When you want to draw random samples from the normal distribution, you can use **rnorm.** 

For example, we could use **rnorm** to simulate random samples from the IQ distribution.

```{r}
# fix random seed for reproducibility
set.seed(1234)

# ?rnorm
obs <- rnorm(n=100, iq.mean, iq.sd) 
mean(obs)




## More Advanced Content: 
## Don't worry if the code below does not totally make sense so far. 
# law of large numbers: mean will approach expected value for large N
n.samples <- c(100, 1000, 10000)
my.df <- do.call(rbind, lapply(n.samples, 
                               function(x) data.frame("SampleSize" = x, 
                                                      "IQ" = rnorm(x, iq.mean, iq.sd))))
# show one facet per random sample of a given size
ggplot() + geom_histogram(data = my.df, aes(x = IQ)) + facet_wrap(.~SampleSize, scales = "free_y")

# note: we can also implement our own sampler using the densities
my.sample <- sample(iq.df$IQ, 100, prob = iq.df$Density, replace = TRUE)
my.sample.df <- data.frame("IQ" = my.sample)
ggplot(my.sample.df, aes(x = IQ)) + geom_histogram()
```


Note that we called **set.seed()** in order to ensure that the random number generator always generates the same sequence of numbers for reproducibility. 

Of the four functions dealing with distributions, **dnorm** is the most important one. This is because the values from pnorm, qnorm, and rnorm are based on dnorm. Still, pnorm, qnorm, and rnorm are very useful convenience functions when dealing with the normal distribution. If you would like to learn about the corresponding functions for the other distributions, you can simply call **?distribtuion** to obtain more information.



## \textcolor{red}{In-class exercises 4.2:}

Please explain what the following code does:

1.
```{r}
pnorm(0)
```

2. 
```{r}
pnorm(1.96, lower.tail=TRUE)
```

3.
```{r}
dnorm(0:10, mean = 0, sd = 1, log = FALSE)
```

4.
```{r}
qnorm(0.025)
```

5. 
```{r}
qnorm(0.975)
```

6. 
```{r}
qnorm(.975, 20, 1.65)
```

7. 
```{r}
rnorm(1:100, 0.05, 0.5)
```




# 3. Descriptive Statistics

## 3.1 Name Commands 
* names() – It works on matrix or data frame objects.
```{r}
names(school_loc)
```

* rownames() – It works on matrix or data frame objects and is used to give names to rows.
```{r output.lines=10}
rownames(school_loc)
```

* colnames() – It works on matrix or data frame objects and is used to give names to columns.
```{r}
colnames(school_loc)
```

* dimnames() – Gets row and column names for matrix or data frame objects, that is, it is used to see dimensions of the data frame.
```{r output.lines=10}
dimnames(school_loc)[1]
```

```{r output.lines=10}
dimnames(school_loc)[2]
```


## 3.2 Summarizing Commands 

* max(x, na.rm = FALSE) – It shows the maximum value. By default, NA values are not removed. NA is considered the largest unless na.rm=true is used.
* min(x, na.rm = FALSE) – Shows minimum value in a vector. If there are na values, NA is returned unless na.rm=true is used.
* length(x) – Gives length of the vector and includes na values. Na.rm=instruction does not work with this command.
* sum(x, na.rm = FALSE) – Shows the sum of the vector elements.
* mean(x, na.rm = FALSE) – We obtain an arithmetic mean with this.
* median( x, na.rm = FALSE) – Shows the median value of the vector.
* sd(x, na.rm = FALSE) – Shows the standard deviation.
* var(x, na.rm = FALSE) – Shows the variance.
* mad(x, na.rm = FALSE) – Shows the median absolute deviation.
* log(dataset) – Shows log value for each element.
* summary(dataset) – We have seen how it shows a summary of dataset like maximum value, minimum value, mean, etc.
* quantile() – Shows the quantiles by default—the 0%, 25%, 50%, 75%, and 100% quantiles. You can select other quantiles also.
```{r}
x <- sample(1:100000, 2000)
quantile(x, probs = seq(0, 1, 0.25), na.rm = FALSE, names = TRUE)
```


We will do more exercises where you would be asked to find these summarizing statistics given a dataframe in the afternoon session. 


## 3.3 Accessing Elements from Summary Statistics

You might want to access elements from the summary statistics sometimes. We will use the built-in dataset longley again to show some examples. 
```{r}
df <- longley
s <- summary(df)
s
# You can simply use indices to access each element like this: 
s[1]
```

When we go into linear regression in QPM I, accessing elements of list-like objects would become more useful. For example, we can fit a linear model using the lm() function where the independent variable is population and the dependent variable is the number of unemployed. 
```{r}
lm <- lm(Unemployed ~ Population, data = df)
```

Let's check the structure of the object: 
```{r output.lines=10}
str(lm)
```
You can use the summary() function again on lm. 
```{r output.lines=10}
summary(lm)
```

If you want to access elements from the summary statistics, you can use dollar signs and brackets, just like we did on Day 2.  
```{r}
lm$coefficients
lm$residuals
lm$coefficients[1] 
lm$coefficients[[1]]
```

## \textcolor{red}{In-class exercises 4.3:}

We will continue using the TFR dataset.

1. Omit all NA's in the dataframe. 
2. Run a linear regression of ChildBearing on TFR.
3. Get the coefficients. 
4. What are the column names and row names? 
5. What is the max and min of Years? 
6. What is the average Life Expectancy at Birth (LifeExpB)?
7. What is the median Life Expectancy at Birth (LifeExpB)?
8. What is the 25% and 75% quantile of Life Expectancy at Birth (LifeExpB)?

```{r eval = FALSE}
TFR <- read.csv("worldTFR.csv")

# 4.
colnames(TFR)
rownames(TFR)

# 5.
max(TFR$Year)
min(TFR$Year)

# 6.
mean(TFR$LifeExpB)

#7. 
median(TFR$LifeExpB)

# 8.
quantile(TFR$LifeExpB)
```








